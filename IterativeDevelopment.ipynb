{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdcnsXiXWsEV"
      },
      "outputs": [],
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from PIL import Image, ImageOps\n",
        "import numpy as np\n",
        "import sklearn.model_selection\n",
        "import os\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsCTG0GpupVL"
      },
      "outputs": [],
      "source": [
        "#For the program to work, data needs to be downloaded (https://drive.google.com/file/d/1hgXutPsaBE7yhqKjMwKy1OtvVQHV4XXk/view?usp=sharing - this is updated file with new photos).\n",
        "#It was found that some problems with uploading big amounts of photos my cause the colab to stop working. The best way to upload photos, is to download the .zip file and upload it\n",
        "#to google drive and use colab api to get the data from the .zip file.\n",
        "#Please, upload the zip file to main directory of your google drive (if not possible, then please change code in next code block).\n",
        "\n",
        "def createDirectories():\n",
        "  #Directories structure (it is checked if directory already exists in order not to erase uploaded images):\n",
        "  drive.mount('drive')\n",
        "  \n",
        "  if not os.path.isdir(\"processedData\"):\n",
        "    os.mkdir(\"processedData\")\n",
        "    \n",
        "createDirectories()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDDTqF_lHAXJ"
      },
      "outputs": [],
      "source": [
        "#This command will extract all the files from .zip file and move them to new directories.\n",
        "#Please check if the location of the file in this code is correct with the location where you uploaded the code on personal google drive.\n",
        "!unzip drive/MyDrive/updatedData.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NE7xk3kbmdA"
      },
      "outputs": [],
      "source": [
        "def preprocessImages(img_cols, img_rows):\n",
        "  #Load data, modify files and save them (COVID)\n",
        "  for i, fileName in enumerate(os.listdir(\"updatedData/CT_COVID\")):\n",
        "    image = Image.open(\"updatedData/CT_COVID/{}\".format(fileName)) #load\n",
        "    resizedImage = image.resize((img_cols, img_rows)) #resize\n",
        "    grayImage = ImageOps.grayscale(resizedImage) #grayscale\n",
        "    grayImage.save(\"processedData/COVID_{}.jpg\".format(i)) #save\n",
        "\n",
        "  #Load data, modify files and save them (NonCOVID)\n",
        "  for i, fileName in enumerate(os.listdir(\"updatedData/CT_NonCOVID\")):\n",
        "    image = Image.open(\"updatedData/CT_NonCOVID/{}\".format(fileName)) #load\n",
        "    resizedImage = image.resize((img_cols, img_rows)) #resize\n",
        "    grayImage = ImageOps.grayscale(resizedImage) #grayscale\n",
        "    grayImage.save(\"processedData/NonCOVID_{}.jpg\".format(i)) #save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGoxbQm3inFD"
      },
      "outputs": [],
      "source": [
        "def createNumpyData(num_classes):\n",
        "  xData = []\n",
        "  yData = []\n",
        "  #Load processed data and append values to arrays\n",
        "  for fileName in os.listdir(\"processedData\"):\n",
        "    image = Image.open('processedData/{}'.format(fileName))\n",
        "    xData.append(np.asarray(image))\n",
        "    if fileName[0] == \"C\":\n",
        "      yData.append(1)\n",
        "    else:\n",
        "      yData.append(0)\n",
        "  \n",
        "  #Convert python array to numpy array\n",
        "  xNumpyData = np.array(xData)\n",
        "  yNumpyData = np.array(yData)\n",
        "  \n",
        "  #Convert pixel values to values between 0 and 1\n",
        "  xNumpyData = xNumpyData.astype('float32')\n",
        "  xNumpyData /= 255\n",
        "  \n",
        "  #Assign classes for yData\n",
        "  yNumpyData = np_utils.to_categorical(yNumpyData, num_classes)\n",
        "\n",
        "  return xNumpyData, yNumpyData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDNCojb0hkvI"
      },
      "outputs": [],
      "source": [
        "def splitData(xNumpyData, yNumpyData, test_size, img_cols, img_rows):\n",
        "  #Split the data\n",
        "  x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(xNumpyData, yNumpyData, test_size=test_size, random_state=42)\n",
        "  #Reshape the xData (gray scale is used so only 1 number needed to describe each pixel)\n",
        "  x_train = x_train.reshape(x_train.shape[0], img_cols, img_rows, 1)\n",
        "  x_test = x_test.reshape(x_test.shape[0], img_cols, img_rows, 1)\n",
        "  input_shape = (img_cols, img_rows, 1)\n",
        "\n",
        "  return x_train, x_test, y_train, y_test, input_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOuBJey8pvm6"
      },
      "outputs": [],
      "source": [
        "#Specify image size and process the images to vectors\n",
        "img_cols, img_rows = 140, 100\n",
        "num_classes = 2\n",
        "\n",
        "preprocessImages(img_cols, img_rows)\n",
        "xNumpyData, yNumpyData = createNumpyData(num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code for the models and augumented training is based on the models used in the workshops - Honglei Li (2022) *KF5012-AI-Stream*. Available at: https://github.com/Hongleili/KF5012-AI-Stream (Accessed: 16 March 2022)."
      ],
      "metadata": {
        "id": "OpeWleATE0Fr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Convolutional Neural Network model"
      ],
      "metadata": {
        "id": "6VRElW68a_yh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1F0j5oh_L0S"
      },
      "outputs": [],
      "source": [
        "def createDeepModel(input_shape, num_classes):\n",
        "  #Create deep model\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, kernel_size=(8, 8),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape, padding='same')) \n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(MaxPooling2D(pool_size=(4, 4))) \n",
        "  model.add(Conv2D(128, (8, 8), activation='relu', padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2))) \n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2))) \n",
        "  model.add(Conv2D(128, (8, 8), activation='relu', padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(num_classes, activation='softmax')) \n",
        "  model.summary()\n",
        "  model.compile(loss=binary_crossentropy,\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standard Convolutional Neural Network model"
      ],
      "metadata": {
        "id": "G7_pjeRvbQIo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmh9e4yTlUHv"
      },
      "outputs": [],
      "source": [
        "def createModel(input_shape, num_classes): \n",
        "  #Create model\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, kernel_size=(8, 8),\n",
        "                   activation='relu',\n",
        "                   input_shape=input_shape))\n",
        "  model.add(Conv2D(64, (8, 8), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(8, 8))) \n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(num_classes, activation='softmax'))\n",
        "  model.summary()\n",
        "  model.compile(loss=binary_crossentropy,\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augumented model training"
      ],
      "metadata": {
        "id": "KfKQwigMbFPR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co_k0L4r9nZn"
      },
      "outputs": [],
      "source": [
        "def trainModelAugumented(model, x_train, y_train, x_test, y_test, batch_size, epochs):    \n",
        "  datagen = ImageDataGenerator(\n",
        "      #Rotate images up to specified value\n",
        "      rotation_range=8,\n",
        "      #Random vertical and horizontal shift up to specified value\n",
        "      width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      #Zoom up to specified value\n",
        "      zoom_range=0.2,\n",
        "      #Allow horizontal flip but not vertical\n",
        "      horizontal_flip=True,\n",
        "      vertical_flip=False)\n",
        "\n",
        "  datagen.fit(x_train)\n",
        "  #Apply augumentation and train the model\n",
        "  model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "            epochs=epochs,\n",
        "            verbose=1,\n",
        "            validation_data=(x_test, y_test))\n",
        "  score = model.evaluate(x_test, y_test, verbose=0)\n",
        "  return score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standard training"
      ],
      "metadata": {
        "id": "C6jNYPCWbkZP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8pmrEsem5Mh"
      },
      "outputs": [],
      "source": [
        "def trainModel(model, x_train, y_train, x_test, y_test, batch_size, epochs):\n",
        "  model.fit(x_train, y_train, batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            verbose=1,\n",
        "            validation_data=(x_test, y_test))\n",
        "  score = model.evaluate(x_test, y_test, verbose=0)\n",
        "  return score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model training"
      ],
      "metadata": {
        "id": "aMW7ctXWcJmu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9ZV9jeiq4zg"
      },
      "outputs": [],
      "source": [
        "#Specify test size and split the data\n",
        "test_size = 0.1\n",
        "x_train, x_val, y_train, y_val, input_shape = splitData(xNumpyData, yNumpyData, test_size, img_cols, img_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One model and one training method can be used in following code block.\n",
        "Standard model with standard training methods were used as they were the most effective."
      ],
      "metadata": {
        "id": "HyyRZ7Lbbt0o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fykVztgonI5o"
      },
      "outputs": [],
      "source": [
        "#Specify learning parameters and fit the model\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "\n",
        "model = createModel(input_shape, num_classes)\n",
        "trainModel(model, x_train, y_train, x_val, y_val, batch_size, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yngx0uHf53e_"
      },
      "outputs": [],
      "source": [
        "#Save model for GUI implementation\n",
        "model.save(\"model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUzV6uRbTZeM"
      },
      "outputs": [],
      "source": [
        "#ZIP the model so it can be downloaded\n",
        "!zip -r model.zip model.h5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Keras Tuner Random Search for Parameter Optimisation\n",
        "Code based on the keras documentation - Keras (no date) *KerasTuner API.* Available at: https://keras.io/api/keras_tuner/ (Accessed: 8 May 2022)."
      ],
      "metadata": {
        "id": "A8QXmM3ES-Pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing keras tuner package and importing\n",
        "!pip install -q -U keras-tuner\n",
        "import keras_tuner as kt\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "PoeVhKLaTCDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instantiating the tuner\n",
        "tuner = kt.RandomSearch(\n",
        "    hypermodel = kt.applications.HyperResNet(include_top=True, input_shape=(img_cols, img_rows, 1), classes=num_classes),\n",
        "    objective = 'val_accuracy',\n",
        "    max_trials = 10,\n",
        "    seed=5,\n",
        "    tune_new_entries=True,\n",
        "    allow_new_entries=True,\n",
        "    project_name = 'automated_accuracy_tuning'\n",
        ")\n",
        "\n",
        "#Stops early after reaching too high validation loss\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
      ],
      "metadata": {
        "id": "gBC70HIlTIPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Runs the tuner on data set (will take a long time)\n",
        "tuner.search(x_train, y_train, epochs=20, validation_data=(x_val, y_val), callbacks=[stop_early])"
      ],
      "metadata": {
        "id": "csDlPPsoTOcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Frabs best models and best hyper parameters and assigns them to variables\n",
        "best_model = tuner.get_best_models(1)[0]\n",
        "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
        "\n",
        "#Prints summary of tuner results\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "Z_g-mPZYTQyb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "IterativeDevelopment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
